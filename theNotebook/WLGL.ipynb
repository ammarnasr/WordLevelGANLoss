{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WLGL.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mVHKOFJKObV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "outputId": "f4977ce0-b563-4d29-d083-65c1928b940a"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon Aug 10 21:31:24 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 450.57       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   41C    P0    28W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X39EBfvPKZkL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.chdir('/content/')\n",
        "!rm -r sample_data\n",
        "#clone repo AttnGAN\n",
        "!git clone https://github.com/taoxugit/AttnGAN.git\n",
        "\n",
        "#Changing Working dirctory to data\n",
        "os.chdir('/content/AttnGAN/data/')\n",
        "#Downloads birds.zip (6.19M) , Extract it , and remove unnesscery files\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1O_LtUP9sch09QH3s_EBAgLEctBQ5JBSJ' -O birds.zip\n",
        "!unzip -q birds.zip\n",
        "!rm birds.zip\n",
        "!rm -r __MACOSX/\n",
        "\n",
        "#Changing Working dirctory to code\n",
        "os.chdir('/content/AttnGAN/code/')\n",
        "#Download Pillow.rar (251.75K), , Extract it , and remove unnesscery files\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1Wr3lQajG7m6Bi3rYFTJb6mwE_d8su111' -O Pillow.rar\n",
        "!unrar x  Pillow.rar\n",
        "!rm Pillow.rar\n",
        "\n",
        "os.chdir('/content/')\n",
        "!git clone https://github.com/ammarnasr/CUB-Attn-GAN.git\n",
        "os.chdir('/content/AttnGAN/DAMSMencoders/')\n",
        "!rm -r bird/\n",
        "os.mkdir('bird')\n",
        "!mv /content/CUB-Attn-GAN/theModel/text_encoder599.pth /content/AttnGAN/DAMSMencoders/bird/\n",
        "!mv /content/CUB-Attn-GAN/theModel/image_encoder599.pth /content/AttnGAN/DAMSMencoders/bird/\n",
        "!rm -r /content/CUB-Attn-GAN/\n",
        "\n",
        "#Changing Working dirctory to birds\n",
        "os.chdir('/content/AttnGAN/data/birds/')\n",
        "!cp '/content/drive/My Drive/cub/CUB_200_2011.tgz' '/content/AttnGAN/data/birds/'\n",
        "!tar zxf  CUB_200_2011.tgz\n",
        "!rm CUB_200_2011.tgz\n",
        "\n",
        "os.chdir('/content')\n",
        "!rm -r WordLevelGANLoss\n",
        "!git clone https://github.com/ammarnasr/WordLevelGANLoss.git\n",
        "!mv /content/WordLevelGANLoss/theCode/GAN/utils.py /content/AttnGAN/code/miscc/\n",
        "!mv /content/WordLevelGANLoss/theCode/GAN/losses.py /content/AttnGAN/code/miscc/\n",
        "!mv /content/WordLevelGANLoss/theCode/GAN/trainer.py /content/AttnGAN/code/\n",
        "!mv /content/WordLevelGANLoss/theCode/GAN/config.py /content/AttnGAN/code/miscc/\n",
        "!mv /content/WordLevelGANLoss/theCode/GAN/datasets.py /content/AttnGAN/code/\n",
        "!mv /content/WordLevelGANLoss/theCode/GAN/bird_attn2.yml /content/AttnGAN/code/cfg/\n",
        "\n",
        "#Checkpoint from drive, edit in bird_attnGAN2.ymal also\n",
        "!cp '/content/drive/My Drive/cubModelGAN/netG_epoch_250.pth' '/content/AttnGAN/models/'\n",
        "!cp '/content/drive/My Drive/cubModelGAN/netD0.pth' '/content/AttnGAN/models/'\n",
        "!cp '/content/drive/My Drive/cubModelGAN/netD1.pth' '/content/AttnGAN/models/'\n",
        "!cp '/content/drive/My Drive/cubModelGAN/netD2.pth' '/content/AttnGAN/models/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXS-eOiZPIQf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "36f540b3-70bc-474d-b2d6-cd81c1b60ea3"
      },
      "source": [
        "os.chdir('/content/AttnGAN/code/')\n",
        "!python main.py           --cfg cfg/bird_attn2.yml --gpu 0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using config:\n",
            "{'B_VALIDATION': False,\n",
            " 'CONFIG_NAME': 'attn2',\n",
            " 'CUDA': True,\n",
            " 'DATASET_NAME': 'birds',\n",
            " 'DATA_DIR': '../data/birds',\n",
            " 'GAN': {'B_ATTENTION': True,\n",
            "         'B_DCGAN': False,\n",
            "         'CONDITION_DIM': 100,\n",
            "         'DF_DIM': 64,\n",
            "         'GF_DIM': 32,\n",
            "         'R_NUM': 2,\n",
            "         'Z_DIM': 100},\n",
            " 'GPU_ID': 0,\n",
            " 'RNN_TYPE': 'LSTM',\n",
            " 'TEXT': {'CAPTIONS_PER_IMAGE': 10, 'EMBEDDING_DIM': 256, 'WORDS_NUM': 18},\n",
            " 'TRAIN': {'BATCH_SIZE': 20,\n",
            "           'B_NET_D': True,\n",
            "           'DISCRIMINATOR_LR': 0.0002,\n",
            "           'ENCODER_LR': 0.0002,\n",
            "           'FLAG': True,\n",
            "           'GENERATOR_LR': 0.0002,\n",
            "           'MAX_EPOCH': 600,\n",
            "           'NET_E': '../DAMSMencoders/bird/text_encoder599.pth',\n",
            "           'NET_G': '../models/netG_epoch_190.pth',\n",
            "           'RNN_GRAD_CLIP': 0.25,\n",
            "           'SMOOTH': {'GAMMA1': 4.0,\n",
            "                      'GAMMA2': 5.0,\n",
            "                      'GAMMA3': 10.0,\n",
            "                      'LAMBDA': 5.0},\n",
            "           'SNAPSHOT_INTERVAL': 10},\n",
            " 'TREE': {'BASE_SIZE': 64, 'BRANCH_NUM': 3},\n",
            " 'WORKERS': 4}\n",
            "/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py:257: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
            "  \"please use transforms.Resize instead.\")\n",
            "Total filenames:  11788 001.Black_footed_Albatross/Black_Footed_Albatross_0046_18.jpg\n",
            "Load filenames from: ../data/birds/train/filenames.pickle (8855)\n",
            "Load filenames from: ../data/birds/test/filenames.pickle (2933)\n",
            "Load from:  ../data/birds/captions.pickle\n",
            "/usr/local/lib/python3.6/dist-packages/torchvision/models/inception.py:77: FutureWarning: The default weight initialization of inception_v3 will be changed in future releases of torchvision. If you wish to keep the old behavior (which leads to long initialization times due to scipy/scipy#11299), please set init_weights=True.\n",
            "  ' due to scipy/scipy#11299), please set init_weights=True.', FutureWarning)\n",
            "Downloading: \"https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth\" to /root/.cache/torch/hub/checkpoints/inception_v3_google-1a9a5a14.pth\n",
            "100% 104M/104M [00:02<00:00, 43.1MB/s]\n",
            "Load pretrained model from  https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth\n",
            "Load image encoder from: ../DAMSMencoders/bird/image_encoder599.pth\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:60: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "Load text encoder from: ../DAMSMencoders/bird/text_encoder599.pth\n",
            "/content/AttnGAN/code/miscc/utils.py:404: UserWarning: nn.init.orthogonal is now deprecated in favor of nn.init.orthogonal_.\n",
            "  nn.init.orthogonal(m.weight.data, 1.0)\n",
            "/content/AttnGAN/code/miscc/utils.py:399: UserWarning: nn.init.orthogonal is now deprecated in favor of nn.init.orthogonal_.\n",
            "  nn.init.orthogonal(m.weight.data, 1.0)\n",
            "# of netsD 3\n",
            "Load G from:  ../models/netG_epoch_190.pth\n",
            "Load D from:  ../models/netD0.pth\n",
            "Load D from:  ../models/netD1.pth\n",
            "Load D from:  ../models/netD2.pth\n",
            "START EPOCH IS 191\n",
            "num_batches :  442\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1625: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
            "/content/AttnGAN/code/GlobalAttention.py:109: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  attn = self.sm(attn)  # Eq. (2)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3121: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
            "  \"See the documentation of nn.Upsample for details.\".format(mode))\n",
            "/content/AttnGAN/code/GlobalAttention.py:51: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  attn = nn.Softmax()(attn)  # Eq. (8)\n",
            "/content/AttnGAN/code/GlobalAttention.py:60: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  attn = nn.Softmax()(attn)\n",
            "[W TensorIterator.cpp:918] Warning: Mixed memory format inputs detected while calling the operator. The operator will output contiguous tensor even if some of the inputs are in channels_last format. (function operator())\n",
            "/content/AttnGAN/code/trainer.py:438: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)\n",
            "  avg_p.mul_(0.999).add_(0.001, p.data)\n",
            "step :  100 iters_100_time :  75.60730981826782\n",
            "step :  200 iters_100_time :  71.27553606033325\n",
            "step :  300 iters_100_time :  71.18063139915466\n",
            "step :  400 iters_100_time :  71.26040577888489\n",
            "[191/600][442]\n",
            "                    Loss_D: 0.11 Loss_G: 53.20 Time: 319.27s\n",
            "num_batches :  442\n",
            "step :  500 iters_100_time :  42.19737458229065\n",
            "step :  600 iters_100_time :  71.10143065452576\n",
            "step :  700 iters_100_time :  71.44432663917542\n",
            "step :  800 iters_100_time :  71.25028467178345\n",
            "[192/600][442]\n",
            "                    Loss_D: 0.06 Loss_G: 44.37 Time: 315.96s\n",
            "num_batches :  442\n",
            "step :  900 iters_100_time :  12.190724611282349\n",
            "step :  1000 iters_100_time :  71.13721513748169\n",
            "step :  1100 iters_100_time :  71.36022925376892\n",
            "step :  1200 iters_100_time :  71.23600888252258\n",
            "step :  1300 iters_100_time :  71.36842393875122\n",
            "[193/600][442]\n",
            "                    Loss_D: 0.79 Loss_G: 47.99 Time: 316.39s\n",
            "num_batches :  442\n",
            "step :  1400 iters_100_time :  53.396676540374756\n",
            "step :  1500 iters_100_time :  71.30898809432983\n",
            "step :  1600 iters_100_time :  71.3185522556305\n",
            "step :  1700 iters_100_time :  71.76704692840576\n",
            "[194/600][442]\n",
            "                    Loss_D: 1.40 Loss_G: 63.17 Time: 316.71s\n",
            "num_batches :  442\n",
            "step :  1800 iters_100_time :  23.75327181816101\n",
            "step :  1900 iters_100_time :  71.45906162261963\n",
            "step :  2000 iters_100_time :  71.40514969825745\n",
            "step :  2100 iters_100_time :  71.6322877407074\n",
            "step :  2200 iters_100_time :  71.8593418598175\n",
            "[195/600][442]\n",
            "                    Loss_D: 0.06 Loss_G: 59.03 Time: 317.70s\n",
            "num_batches :  442\n",
            "step :  2300 iters_100_time :  65.01910758018494\n",
            "step :  2400 iters_100_time :  71.64498019218445\n",
            "step :  2500 iters_100_time :  71.50935959815979\n",
            "step :  2600 iters_100_time :  71.54630899429321\n",
            "[196/600][442]\n",
            "                    Loss_D: 0.43 Loss_G: 57.69 Time: 317.40s\n",
            "num_batches :  442\n",
            "step :  2700 iters_100_time :  35.454633951187134\n",
            "step :  2800 iters_100_time :  71.62397909164429\n",
            "step :  2900 iters_100_time :  71.63096117973328\n",
            "step :  3000 iters_100_time :  71.69189548492432\n",
            "[197/600][442]\n",
            "                    Loss_D: 1.14 Loss_G: 51.46 Time: 318.00s\n",
            "num_batches :  442\n",
            "step :  3100 iters_100_time :  4.96755576133728\n",
            "step :  3200 iters_100_time :  71.68025588989258\n",
            "step :  3300 iters_100_time :  71.66514372825623\n",
            "step :  3400 iters_100_time :  71.7728521823883\n",
            "step :  3500 iters_100_time :  71.68036270141602\n",
            "[198/600][442]\n",
            "                    Loss_D: 0.33 Loss_G: 56.76 Time: 317.94s\n",
            "num_batches :  442\n",
            "step :  3600 iters_100_time :  46.416175365448\n",
            "step :  3700 iters_100_time :  71.90863490104675\n",
            "step :  3800 iters_100_time :  71.39530038833618\n",
            "step :  3900 iters_100_time :  71.81188440322876\n",
            "[199/600][442]\n",
            "                    Loss_D: 0.24 Loss_G: 59.64 Time: 317.64s\n",
            "num_batches :  442\n",
            "step :  4000 iters_100_time :  16.601519107818604\n",
            "step :  4100 iters_100_time :  71.74508118629456\n",
            "step :  4200 iters_100_time :  71.76916599273682\n",
            "step :  4300 iters_100_time :  71.65314745903015\n",
            "step :  4400 iters_100_time :  71.85839557647705\n",
            "[200/600][442]\n",
            "                    Loss_D: 0.37 Loss_G: 60.12 Time: 318.24s\n",
            "Save G/Ds models.\n",
            "num_batches :  442\n",
            "step :  4500 iters_100_time :  58.53235936164856\n",
            "step :  4600 iters_100_time :  71.75300931930542\n",
            "step :  4700 iters_100_time :  71.5712161064148\n",
            "step :  4800 iters_100_time :  71.87564158439636\n",
            "[201/600][442]\n",
            "                    Loss_D: 0.07 Loss_G: 57.11 Time: 318.56s\n",
            "num_batches :  442\n",
            "step :  4900 iters_100_time :  27.95580768585205\n",
            "step :  5000 iters_100_time :  71.84553146362305\n",
            "step :  5000 iters_5000_time :  99.80140829086304\n",
            "CURRENT WORKING DIRCTORY :  /content/AttnGAN/code\n",
            "keyTime |||||||||||||||||||||||||||||||\n",
            "build_super_images_time :  7.3838701248168945\n",
            "KeyTime |||||||||||||||||||||||||||||||\n",
            "CURRENT WORKING DIRCTORY :  /content/AttnGAN/code\n",
            "keyTime |||||||||||||||||||||||||||||||\n",
            "build_super_images_time :  27.41123867034912\n",
            "KeyTime |||||||||||||||||||||||||||||||\n",
            "CURRENT WORKING DIRCTORY :  /content/AttnGAN/code\n",
            "keyTime |||||||||||||||||||||||||||||||\n",
            "build_super_images_time :  209.40354681015015\n",
            "KeyTime |||||||||||||||||||||||||||||||\n",
            "step :  5100 iters_100_time :  338.3681137561798\n",
            "step :  5200 iters_100_time :  71.84741449356079\n",
            "step :  5300 iters_100_time :  71.65962052345276\n",
            "[202/600][442]\n",
            "                    Loss_D: 2.65 Loss_G: 70.61 Time: 584.72s\n",
            "num_batches :  442\n",
            "step :  5400 iters_100_time :  69.4885561466217\n",
            "step :  5500 iters_100_time :  71.98630475997925\n",
            "step :  5600 iters_100_time :  71.59996175765991\n",
            "step :  5700 iters_100_time :  71.83474016189575\n",
            "[203/600][442]\n",
            "                    Loss_D: 0.53 Loss_G: 47.51 Time: 318.40s\n",
            "num_batches :  442\n",
            "step :  5800 iters_100_time :  39.557883739471436\n",
            "step :  5900 iters_100_time :  71.93798017501831\n",
            "step :  6000 iters_100_time :  71.90103077888489\n",
            "step :  6100 iters_100_time :  72.17728400230408\n",
            "[204/600][442]\n",
            "                    Loss_D: 0.13 Loss_G: 60.47 Time: 319.10s\n",
            "num_batches :  442\n",
            "step :  6200 iters_100_time :  9.465893507003784\n",
            "step :  6300 iters_100_time :  71.8797218799591\n",
            "step :  6400 iters_100_time :  71.9142017364502\n",
            "step :  6500 iters_100_time :  71.99232578277588\n",
            "step :  6600 iters_100_time :  72.06320142745972\n",
            "[205/600][442]\n",
            "                    Loss_D: 0.17 Loss_G: 39.18 Time: 319.06s\n",
            "num_batches :  442\n",
            "step :  6700 iters_100_time :  50.93809223175049\n",
            "step :  6800 iters_100_time :  71.8227150440216\n",
            "step :  6900 iters_100_time :  71.77435517311096\n",
            "step :  7000 iters_100_time :  71.74714779853821\n",
            "[206/600][442]\n",
            "                    Loss_D: 0.66 Loss_G: 67.31 Time: 318.19s\n",
            "num_batches :  442\n",
            "step :  7100 iters_100_time :  20.950554370880127\n",
            "step :  7200 iters_100_time :  71.76937413215637\n",
            "step :  7300 iters_100_time :  71.66381764411926\n",
            "step :  7400 iters_100_time :  71.96792221069336\n",
            "step :  7500 iters_100_time :  71.79530358314514\n",
            "[207/600][442]\n",
            "                    Loss_D: 0.06 Loss_G: 61.86 Time: 318.50s\n",
            "num_batches :  442\n",
            "step :  7600 iters_100_time :  62.36253833770752\n",
            "step :  7700 iters_100_time :  71.6125807762146\n",
            "step :  7800 iters_100_time :  72.05641722679138\n",
            "step :  7900 iters_100_time :  71.71332216262817\n",
            "[208/600][442]\n",
            "                    Loss_D: 0.11 Loss_G: 62.16 Time: 318.48s\n",
            "num_batches :  442\n",
            "step :  8000 iters_100_time :  32.18353486061096\n",
            "step :  8100 iters_100_time :  71.82540082931519\n",
            "step :  8200 iters_100_time :  71.82609724998474\n",
            "step :  8300 iters_100_time :  71.77362513542175\n",
            "[209/600][442]\n",
            "                    Loss_D: 0.07 Loss_G: 52.13 Time: 317.91s\n",
            "num_batches :  442\n",
            "step :  8400 iters_100_time :  2.1573901176452637\n",
            "step :  8500 iters_100_time :  71.99390602111816\n",
            "step :  8600 iters_100_time :  71.74241185188293\n",
            "step :  8700 iters_100_time :  71.87093091011047\n",
            "step :  8800 iters_100_time :  71.80699896812439\n",
            "[210/600][442]\n",
            "                    Loss_D: 1.01 Loss_G: 47.05 Time: 318.80s\n",
            "Save G/Ds models.\n",
            "num_batches :  442\n",
            "step :  8900 iters_100_time :  44.51836156845093\n",
            "step :  9000 iters_100_time :  71.95768404006958\n",
            "step :  9100 iters_100_time :  71.61660385131836\n",
            "step :  9200 iters_100_time :  71.68328356742859\n",
            "[211/600][442]\n",
            "                    Loss_D: 0.96 Loss_G: 35.27 Time: 319.29s\n",
            "num_batches :  442\n",
            "step :  9300 iters_100_time :  13.483056783676147\n",
            "step :  9400 iters_100_time :  71.53735542297363\n",
            "step :  9500 iters_100_time :  71.79732155799866\n",
            "step :  9600 iters_100_time :  71.75429797172546\n",
            "step :  9700 iters_100_time :  71.74440383911133\n",
            "[212/600][442]\n",
            "                    Loss_D: 0.47 Loss_G: 50.31 Time: 317.91s\n",
            "num_batches :  442\n",
            "step :  9800 iters_100_time :  55.2089409828186\n",
            "step :  9900 iters_100_time :  71.64824080467224\n",
            "step :  10000 iters_100_time :  71.76831650733948\n",
            "step :  10000 iters_5000_time :  198.62561321258545\n",
            "CURRENT WORKING DIRCTORY :  /content/AttnGAN/code\n",
            "keyTime |||||||||||||||||||||||||||||||\n",
            "build_super_images_time :  7.013991594314575\n",
            "KeyTime |||||||||||||||||||||||||||||||\n",
            "CURRENT WORKING DIRCTORY :  /content/AttnGAN/code\n",
            "keyTime |||||||||||||||||||||||||||||||\n",
            "build_super_images_time :  26.31724190711975\n",
            "KeyTime |||||||||||||||||||||||||||||||\n",
            "CURRENT WORKING DIRCTORY :  /content/AttnGAN/code\n",
            "keyTime |||||||||||||||||||||||||||||||\n",
            "build_super_images_time :  213.74221920967102\n",
            "KeyTime |||||||||||||||||||||||||||||||\n",
            "step :  10100 iters_100_time :  340.20367884635925\n",
            "[213/600][442]\n",
            "                    Loss_D: 0.07 Loss_G: 58.46 Time: 586.57s\n",
            "num_batches :  442\n",
            "step :  10200 iters_100_time :  24.835506916046143\n",
            "step :  10300 iters_100_time :  71.6107804775238\n",
            "step :  10400 iters_100_time :  71.57508063316345\n",
            "step :  10500 iters_100_time :  71.80174446105957\n",
            "step :  10600 iters_100_time :  71.86019539833069\n",
            "[214/600][442]\n",
            "                    Loss_D: 2.12 Loss_G: 71.97 Time: 317.57s\n",
            "num_batches :  442\n",
            "step :  10700 iters_100_time :  66.62299942970276\n",
            "step :  10800 iters_100_time :  71.25273990631104\n",
            "step :  10900 iters_100_time :  71.50141406059265\n",
            "step :  11000 iters_100_time :  71.23419427871704\n",
            "[215/600][442]\n",
            "                    Loss_D: 0.44 Loss_G: 60.68 Time: 316.74s\n",
            "num_batches :  442\n",
            "step :  11100 iters_100_time :  36.47837424278259\n",
            "step :  11200 iters_100_time :  71.5567741394043\n",
            "step :  11300 iters_100_time :  71.41475319862366\n",
            "step :  11400 iters_100_time :  71.55502009391785\n",
            "[216/600][442]\n",
            "                    Loss_D: 0.96 Loss_G: 58.71 Time: 317.27s\n",
            "num_batches :  442\n",
            "step :  11500 iters_100_time :  6.249480247497559\n",
            "step :  11600 iters_100_time :  71.84011769294739\n",
            "step :  11700 iters_100_time :  71.24298620223999\n",
            "step :  11800 iters_100_time :  71.64387941360474\n",
            "step :  11900 iters_100_time :  71.33284187316895\n",
            "[217/600][442]\n",
            "                    Loss_D: 0.32 Loss_G: 64.77 Time: 317.04s\n",
            "num_batches :  442\n",
            "step :  12000 iters_100_time :  47.76176476478577\n",
            "step :  12100 iters_100_time :  71.56469798088074\n",
            "step :  12200 iters_100_time :  71.26149535179138\n",
            "step :  12300 iters_100_time :  71.65729570388794\n",
            "[218/600][442]\n",
            "                    Loss_D: 0.20 Loss_G: 46.81 Time: 317.09s\n",
            "num_batches :  442\n",
            "step :  12400 iters_100_time :  17.673470735549927\n",
            "step :  12500 iters_100_time :  71.532146692276\n",
            "step :  12600 iters_100_time :  71.9306743144989\n",
            "step :  12700 iters_100_time :  71.44920134544373\n",
            "step :  12800 iters_100_time :  71.80051302909851\n",
            "[219/600][442]\n",
            "                    Loss_D: 1.17 Loss_G: 67.88 Time: 317.45s\n",
            "num_batches :  442\n",
            "step :  12900 iters_100_time :  59.12432050704956\n",
            "step :  13000 iters_100_time :  71.64762711524963\n",
            "step :  13100 iters_100_time :  71.64869594573975\n",
            "step :  13200 iters_100_time :  71.69229030609131\n",
            "[220/600][442]\n",
            "                    Loss_D: 2.74 Loss_G: 66.37 Time: 317.79s\n",
            "Save G/Ds models.\n",
            "num_batches :  442\n",
            "step :  13300 iters_100_time :  29.519957065582275\n",
            "step :  13400 iters_100_time :  71.51927876472473\n",
            "step :  13500 iters_100_time :  71.80323529243469\n",
            "step :  13600 iters_100_time :  71.3804202079773\n",
            "step :  13700 iters_100_time :  71.43010473251343\n",
            "[221/600][442]\n",
            "                    Loss_D: 0.15 Loss_G: 50.89 Time: 317.63s\n",
            "num_batches :  442\n",
            "step :  13800 iters_100_time :  70.6538872718811\n",
            "step :  13900 iters_100_time :  71.57078051567078\n",
            "step :  14000 iters_100_time :  71.68052172660828\n",
            "step :  14100 iters_100_time :  71.49238443374634\n",
            "[222/600][442]\n",
            "                    Loss_D: 0.59 Loss_G: 38.33 Time: 317.38s\n",
            "num_batches :  442\n",
            "step :  14200 iters_100_time :  40.692580461502075\n",
            "step :  14300 iters_100_time :  71.83058452606201\n",
            "step :  14400 iters_100_time :  71.61450910568237\n",
            "step :  14500 iters_100_time :  71.54072690010071\n",
            "[223/600][442]\n",
            "                    Loss_D: 0.43 Loss_G: 55.28 Time: 317.50s\n",
            "num_batches :  442\n",
            "step :  14600 iters_100_time :  10.767776727676392\n",
            "step :  14700 iters_100_time :  71.54998278617859\n",
            "step :  14800 iters_100_time :  71.5146427154541\n",
            "step :  14900 iters_100_time :  71.69355630874634\n",
            "step :  15000 iters_100_time :  71.56695604324341\n",
            "step :  15000 iters_5000_time :  297.0930893421173\n",
            "CURRENT WORKING DIRCTORY :  /content/AttnGAN/code\n",
            "keyTime |||||||||||||||||||||||||||||||\n",
            "build_super_images_time :  6.943674564361572\n",
            "KeyTime |||||||||||||||||||||||||||||||\n",
            "CURRENT WORKING DIRCTORY :  /content/AttnGAN/code\n",
            "keyTime |||||||||||||||||||||||||||||||\n",
            "build_super_images_time :  26.075735569000244\n",
            "KeyTime |||||||||||||||||||||||||||||||\n",
            "CURRENT WORKING DIRCTORY :  /content/AttnGAN/code\n",
            "keyTime |||||||||||||||||||||||||||||||\n",
            "build_super_images_time :  207.10352611541748\n",
            "KeyTime |||||||||||||||||||||||||||||||\n",
            "[224/600][442]\n",
            "                    Loss_D: 0.35 Loss_G: 48.59 Time: 578.50s\n",
            "num_batches :  442\n",
            "step :  15100 iters_100_time :  52.34465289115906\n",
            "step :  15200 iters_100_time :  71.65497040748596\n",
            "step :  15300 iters_100_time :  71.64393854141235\n",
            "step :  15400 iters_100_time :  71.50952100753784\n",
            "[225/600][442]\n",
            "                    Loss_D: 0.42 Loss_G: 59.03 Time: 317.93s\n",
            "num_batches :  442\n",
            "step :  15500 iters_100_time :  21.937729597091675\n",
            "step :  15600 iters_100_time :  71.56142735481262\n",
            "step :  15700 iters_100_time :  71.71389651298523\n",
            "step :  15800 iters_100_time :  72.07337951660156\n",
            "step :  15900 iters_100_time :  71.42649412155151\n",
            "[226/600][442]\n",
            "                    Loss_D: 0.29 Loss_G: 44.95 Time: 317.97s\n",
            "num_batches :  442\n",
            "step :  16000 iters_100_time :  63.698095083236694\n",
            "step :  16100 iters_100_time :  71.75125193595886\n",
            "step :  16200 iters_100_time :  71.5274555683136\n",
            "step :  16300 iters_100_time :  71.71190071105957\n",
            "[227/600][442]\n",
            "                    Loss_D: 0.15 Loss_G: 60.99 Time: 317.80s\n",
            "num_batches :  442\n",
            "step :  16400 iters_100_time :  33.665642499923706\n",
            "step :  16500 iters_100_time :  71.47946619987488\n",
            "step :  16600 iters_100_time :  71.74476885795593\n",
            "step :  16700 iters_100_time :  71.75256037712097\n",
            "[228/600][442]\n",
            "                    Loss_D: 0.12 Loss_G: 54.67 Time: 318.04s\n",
            "num_batches :  442\n",
            "step :  16800 iters_100_time :  3.3305420875549316\n",
            "step :  16900 iters_100_time :  71.71473836898804\n",
            "step :  17000 iters_100_time :  72.04911279678345\n",
            "step :  17100 iters_100_time :  71.53779077529907\n",
            "step :  17200 iters_100_time :  71.95668983459473\n",
            "[229/600][442]\n",
            "                    Loss_D: 0.07 Loss_G: 48.42 Time: 317.91s\n",
            "num_batches :  442\n",
            "step :  17300 iters_100_time :  45.25649976730347\n",
            "step :  17400 iters_100_time :  71.74597978591919\n",
            "step :  17500 iters_100_time :  72.02204513549805\n",
            "step :  17600 iters_100_time :  71.55392789840698\n",
            "[230/600][442]\n",
            "                    Loss_D: 0.05 Loss_G: 54.16 Time: 318.70s\n",
            "Save G/Ds models.\n",
            "num_batches :  442\n",
            "step :  17700 iters_100_time :  14.927586317062378\n",
            "step :  17800 iters_100_time :  71.69186425209045\n",
            "step :  17900 iters_100_time :  71.98053669929504\n",
            "step :  18000 iters_100_time :  71.90099740028381\n",
            "step :  18100 iters_100_time :  71.59557342529297\n",
            "[231/600][442]\n",
            "                    Loss_D: 0.33 Loss_G: 49.66 Time: 318.39s\n",
            "num_batches :  442\n",
            "step :  18200 iters_100_time :  56.667139768600464\n",
            "step :  18300 iters_100_time :  71.89359378814697\n",
            "step :  18400 iters_100_time :  71.50125861167908\n",
            "step :  18500 iters_100_time :  71.6393187046051\n",
            "[232/600][442]\n",
            "                    Loss_D: 0.45 Loss_G: 50.96 Time: 318.16s\n",
            "num_batches :  442\n",
            "step :  18600 iters_100_time :  26.205060720443726\n",
            "step :  18700 iters_100_time :  71.79011535644531\n",
            "step :  18800 iters_100_time :  71.69680666923523\n",
            "step :  18900 iters_100_time :  72.18046808242798\n",
            "step :  19000 iters_100_time :  71.8767237663269\n",
            "[233/600][442]\n",
            "                    Loss_D: 0.15 Loss_G: 52.35 Time: 318.44s\n",
            "num_batches :  442\n",
            "step :  19100 iters_100_time :  68.11547994613647\n",
            "step :  19200 iters_100_time :  71.72326874732971\n",
            "step :  19300 iters_100_time :  71.9291558265686\n",
            "step :  19400 iters_100_time :  71.88098335266113\n",
            "[234/600][442]\n",
            "                    Loss_D: 1.24 Loss_G: 63.93 Time: 318.72s\n",
            "num_batches :  442\n",
            "step :  19500 iters_100_time :  37.84658098220825\n",
            "step :  19600 iters_100_time :  71.87312412261963\n",
            "step :  19700 iters_100_time :  71.99938344955444\n",
            "step :  19800 iters_100_time :  72.09401273727417\n",
            "[235/600][442]\n",
            "                    Loss_D: 0.54 Loss_G: 54.01 Time: 318.94s\n",
            "num_batches :  442\n",
            "step :  19900 iters_100_time :  7.979902029037476\n",
            "step :  20000 iters_100_time :  72.26964998245239\n",
            "step :  20000 iters_5000_time :  80.24962973594666\n",
            "CURRENT WORKING DIRCTORY :  /content/AttnGAN/code\n",
            "keyTime |||||||||||||||||||||||||||||||\n",
            "build_super_images_time :  7.025178670883179\n",
            "KeyTime |||||||||||||||||||||||||||||||\n",
            "CURRENT WORKING DIRCTORY :  /content/AttnGAN/code\n",
            "keyTime |||||||||||||||||||||||||||||||\n",
            "build_super_images_time :  26.015698432922363\n",
            "KeyTime |||||||||||||||||||||||||||||||\n",
            "CURRENT WORKING DIRCTORY :  /content/AttnGAN/code\n",
            "keyTime |||||||||||||||||||||||||||||||\n",
            "build_super_images_time :  203.55483150482178\n",
            "KeyTime |||||||||||||||||||||||||||||||\n",
            "step :  20100 iters_100_time :  329.1318745613098\n",
            "step :  20200 iters_100_time :  72.04228353500366\n",
            "step :  20300 iters_100_time :  71.7323579788208\n",
            "[236/600][442]\n",
            "                    Loss_D: 1.14 Loss_G: 63.34 Time: 576.86s\n",
            "num_batches :  442\n",
            "step :  20400 iters_100_time :  49.522356033325195\n",
            "step :  20500 iters_100_time :  72.10230493545532\n",
            "step :  20600 iters_100_time :  71.94608283042908\n",
            "step :  20700 iters_100_time :  72.42394065856934\n",
            "[237/600][442]\n",
            "                    Loss_D: 0.19 Loss_G: 68.04 Time: 319.37s\n",
            "num_batches :  442\n",
            "step :  20800 iters_100_time :  19.25922703742981\n",
            "step :  20900 iters_100_time :  71.8296856880188\n",
            "step :  21000 iters_100_time :  72.07909202575684\n",
            "step :  21100 iters_100_time :  71.84466028213501\n",
            "step :  21200 iters_100_time :  72.00824570655823\n",
            "[238/600][442]\n",
            "                    Loss_D: 1.99 Loss_G: 36.09 Time: 319.03s\n",
            "num_batches :  442\n",
            "step :  21300 iters_100_time :  61.09429693222046\n",
            "step :  21400 iters_100_time :  72.07710933685303\n",
            "step :  21500 iters_100_time :  71.88761258125305\n",
            "step :  21600 iters_100_time :  72.2609589099884\n",
            "[239/600][442]\n",
            "                    Loss_D: 0.20 Loss_G: 47.27 Time: 319.46s\n",
            "num_batches :  442\n",
            "step :  21700 iters_100_time :  30.7425856590271\n",
            "step :  21800 iters_100_time :  71.91353130340576\n",
            "step :  21900 iters_100_time :  72.07871127128601\n",
            "step :  22000 iters_100_time :  71.9017641544342\n",
            "step :  22100 iters_100_time :  72.06259250640869\n",
            "[240/600][442]\n",
            "                    Loss_D: 0.32 Loss_G: 53.77 Time: 319.27s\n",
            "Save G/Ds models.\n",
            "num_batches :  442\n",
            "step :  22200 iters_100_time :  72.93084597587585\n",
            "step :  22300 iters_100_time :  72.20512771606445\n",
            "step :  22400 iters_100_time :  72.08238697052002\n",
            "step :  22500 iters_100_time :  71.86006021499634\n",
            "[241/600][442]\n",
            "                    Loss_D: 0.14 Loss_G: 51.05 Time: 319.84s\n",
            "num_batches :  442\n",
            "step :  22600 iters_100_time :  42.48519325256348\n",
            "step :  22700 iters_100_time :  72.18689489364624\n",
            "step :  22800 iters_100_time :  71.8678662776947\n",
            "step :  22900 iters_100_time :  72.2734637260437\n",
            "[242/600][442]\n",
            "                    Loss_D: 0.28 Loss_G: 60.45 Time: 319.60s\n",
            "num_batches :  442\n",
            "step :  23000 iters_100_time :  12.19722056388855\n",
            "step :  23100 iters_100_time :  72.25755095481873\n",
            "step :  23200 iters_100_time :  72.02348899841309\n",
            "step :  23300 iters_100_time :  72.28572344779968\n",
            "step :  23400 iters_100_time :  72.16710233688354\n",
            "[243/600][442]\n",
            "                    Loss_D: 3.63 Loss_G: 86.32 Time: 319.95s\n",
            "num_batches :  442\n",
            "step :  23500 iters_100_time :  53.97696661949158\n",
            "step :  23600 iters_100_time :  72.29486918449402\n",
            "step :  23700 iters_100_time :  71.90008115768433\n",
            "step :  23800 iters_100_time :  72.16634249687195\n",
            "[244/600][442]\n",
            "                    Loss_D: 0.03 Loss_G: 56.45 Time: 319.94s\n",
            "num_batches :  442\n",
            "step :  23900 iters_100_time :  23.364147663116455\n",
            "step :  24000 iters_100_time :  72.07445526123047\n",
            "step :  24100 iters_100_time :  72.25479316711426\n",
            "step :  24200 iters_100_time :  72.04460072517395\n",
            "step :  24300 iters_100_time :  72.22486114501953\n",
            "[245/600][442]\n",
            "                    Loss_D: 0.48 Loss_G: 63.19 Time: 319.34s\n",
            "num_batches :  442\n",
            "step :  24400 iters_100_time :  65.61312747001648\n",
            "step :  24500 iters_100_time :  72.27860593795776\n",
            "step :  24600 iters_100_time :  72.29882645606995\n",
            "step :  24700 iters_100_time :  72.07011866569519\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0i8h7SFRIoJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}