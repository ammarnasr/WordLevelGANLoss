{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WLGL.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mVHKOFJKObV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X39EBfvPKZkL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.chdir('/content/')\n",
        "!rm -r sample_data\n",
        "#clone repo AttnGAN\n",
        "!git clone https://github.com/taoxugit/AttnGAN.git\n",
        "\n",
        "#Changing Working dirctory to data\n",
        "os.chdir('/content/AttnGAN/data/')\n",
        "#Downloads birds.zip (6.19M) , Extract it , and remove unnesscery files\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1O_LtUP9sch09QH3s_EBAgLEctBQ5JBSJ' -O birds.zip\n",
        "!unzip -q birds.zip\n",
        "!rm birds.zip\n",
        "!rm -r __MACOSX/\n",
        "\n",
        "#Changing Working dirctory to code\n",
        "os.chdir('/content/AttnGAN/code/')\n",
        "#Download Pillow.rar (251.75K), , Extract it , and remove unnesscery files\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1Wr3lQajG7m6Bi3rYFTJb6mwE_d8su111' -O Pillow.rar\n",
        "!unrar x  Pillow.rar\n",
        "!rm Pillow.rar\n",
        "\n",
        "os.chdir('/content/')\n",
        "!git clone https://github.com/ammarnasr/CUB-Attn-GAN.git\n",
        "os.chdir('/content/AttnGAN/DAMSMencoders/')\n",
        "!rm -r bird/\n",
        "os.mkdir('bird')\n",
        "!mv /content/CUB-Attn-GAN/theModel/text_encoder599.pth /content/AttnGAN/DAMSMencoders/bird/\n",
        "!mv /content/CUB-Attn-GAN/theModel/image_encoder599.pth /content/AttnGAN/DAMSMencoders/bird/\n",
        "!rm -r /content/CUB-Attn-GAN/\n",
        "\n",
        "#Changing Working dirctory to birds\n",
        "os.chdir('/content/AttnGAN/data/birds/')\n",
        "!cp '/content/drive/My Drive/cub/CUB_200_2011.tgz' '/content/AttnGAN/data/birds/'\n",
        "!tar zxf  CUB_200_2011.tgz\n",
        "!rm CUB_200_2011.tgz\n",
        "\n",
        "os.chdir('/content')\n",
        "!rm -r WordLevelGANLoss\n",
        "!git clone https://github.com/ammarnasr/WordLevelGANLoss.git\n",
        "!mv /content/WordLevelGANLoss/theCode/GAN/utils.py /content/AttnGAN/code/miscc/\n",
        "!mv /content/WordLevelGANLoss/theCode/GAN/losses.py /content/AttnGAN/code/miscc/\n",
        "!mv /content/WordLevelGANLoss/theCode/GAN/trainer.py /content/AttnGAN/code/\n",
        "!mv /content/WordLevelGANLoss/theCode/GAN/config.py /content/AttnGAN/code/miscc/\n",
        "!mv /content/WordLevelGANLoss/theCode/GAN/datasets.py /content/AttnGAN/code/\n",
        "!mv /content/WordLevelGANLoss/theCode/GAN/bird_attn2.yml /content/AttnGAN/code/cfg/\n",
        "\n",
        "#Checkpoint from drive, edit in bird_attnGAN2.ymal also\n",
        "!cp '/content/drive/My Drive/cubModelGAN/netG_epoch_390.pth' '/content/AttnGAN/models/'\n",
        "!cp '/content/drive/My Drive/cubModelGAN/netD0.pth' '/content/AttnGAN/models/'\n",
        "!cp '/content/drive/My Drive/cubModelGAN/netD1.pth' '/content/AttnGAN/models/'\n",
        "!cp '/content/drive/My Drive/cubModelGAN/netD2.pth' '/content/AttnGAN/models/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXS-eOiZPIQf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d8e1eab5-e135-46b3-cb8d-f3c35342199a"
      },
      "source": [
        "os.chdir('/content/AttnGAN/code/')\n",
        "!python main.py           --cfg cfg/bird_attn2.yml --gpu 0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using config:\n",
            "{'B_VALIDATION': False,\n",
            " 'CONFIG_NAME': 'attn2',\n",
            " 'CUDA': True,\n",
            " 'DATASET_NAME': 'birds',\n",
            " 'DATA_DIR': '../data/birds',\n",
            " 'GAN': {'B_ATTENTION': True,\n",
            "         'B_DCGAN': False,\n",
            "         'CONDITION_DIM': 100,\n",
            "         'DF_DIM': 64,\n",
            "         'GF_DIM': 32,\n",
            "         'R_NUM': 2,\n",
            "         'Z_DIM': 100},\n",
            " 'GPU_ID': 0,\n",
            " 'RNN_TYPE': 'LSTM',\n",
            " 'TEXT': {'CAPTIONS_PER_IMAGE': 10, 'EMBEDDING_DIM': 256, 'WORDS_NUM': 18},\n",
            " 'TRAIN': {'BATCH_SIZE': 20,\n",
            "           'B_NET_D': True,\n",
            "           'DISCRIMINATOR_LR': 0.0002,\n",
            "           'ENCODER_LR': 0.0002,\n",
            "           'FLAG': True,\n",
            "           'GENERATOR_LR': 0.0002,\n",
            "           'MAX_EPOCH': 600,\n",
            "           'NET_E': '../DAMSMencoders/bird/text_encoder599.pth',\n",
            "           'NET_G': '../models/netG_epoch_350.pth',\n",
            "           'RNN_GRAD_CLIP': 0.25,\n",
            "           'SMOOTH': {'GAMMA1': 4.0,\n",
            "                      'GAMMA2': 5.0,\n",
            "                      'GAMMA3': 10.0,\n",
            "                      'LAMBDA': 5.0},\n",
            "           'SNAPSHOT_INTERVAL': 10},\n",
            " 'TREE': {'BASE_SIZE': 64, 'BRANCH_NUM': 3},\n",
            " 'WORKERS': 4}\n",
            "/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py:257: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
            "  \"please use transforms.Resize instead.\")\n",
            "Total filenames:  11788 001.Black_footed_Albatross/Black_Footed_Albatross_0046_18.jpg\n",
            "Load filenames from: ../data/birds/train/filenames.pickle (8855)\n",
            "Load filenames from: ../data/birds/test/filenames.pickle (2933)\n",
            "Load from:  ../data/birds/captions.pickle\n",
            "/usr/local/lib/python3.6/dist-packages/torchvision/models/inception.py:77: FutureWarning: The default weight initialization of inception_v3 will be changed in future releases of torchvision. If you wish to keep the old behavior (which leads to long initialization times due to scipy/scipy#11299), please set init_weights=True.\n",
            "  ' due to scipy/scipy#11299), please set init_weights=True.', FutureWarning)\n",
            "Downloading: \"https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth\" to /root/.cache/torch/hub/checkpoints/inception_v3_google-1a9a5a14.pth\n",
            "100% 104M/104M [00:01<00:00, 78.8MB/s]\n",
            "Load pretrained model from  https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth\n",
            "Load image encoder from: ../DAMSMencoders/bird/image_encoder599.pth\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:60: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "Load text encoder from: ../DAMSMencoders/bird/text_encoder599.pth\n",
            "/content/AttnGAN/code/miscc/utils.py:404: UserWarning: nn.init.orthogonal is now deprecated in favor of nn.init.orthogonal_.\n",
            "  nn.init.orthogonal(m.weight.data, 1.0)\n",
            "/content/AttnGAN/code/miscc/utils.py:399: UserWarning: nn.init.orthogonal is now deprecated in favor of nn.init.orthogonal_.\n",
            "  nn.init.orthogonal(m.weight.data, 1.0)\n",
            "# of netsD 3\n",
            "Load G from:  ../models/netG_epoch_350.pth\n",
            "Load D from:  ../models/netD0.pth\n",
            "Load D from:  ../models/netD1.pth\n",
            "Load D from:  ../models/netD2.pth\n",
            "START EPOCH IS 351\n",
            "num_batches :  442\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1625: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
            "/content/AttnGAN/code/GlobalAttention.py:109: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  attn = self.sm(attn)  # Eq. (2)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3121: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
            "  \"See the documentation of nn.Upsample for details.\".format(mode))\n",
            "/content/AttnGAN/code/GlobalAttention.py:51: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  attn = nn.Softmax()(attn)  # Eq. (8)\n",
            "/content/AttnGAN/code/GlobalAttention.py:60: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  attn = nn.Softmax()(attn)\n",
            "[W TensorIterator.cpp:918] Warning: Mixed memory format inputs detected while calling the operator. The operator will output contiguous tensor even if some of the inputs are in channels_last format. (function operator())\n",
            "/content/AttnGAN/code/trainer.py:438: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)\n",
            "  avg_p.mul_(0.999).add_(0.001, p.data)\n",
            "step :  100 iters_100_time :  80.13817167282104\n",
            "step :  200 iters_100_time :  75.09151411056519\n",
            "step :  300 iters_100_time :  75.01821327209473\n",
            "step :  400 iters_100_time :  75.4111795425415\n",
            "[351/600][442]\n",
            "                    Loss_D: 0.24 Loss_G: 77.49 Time: 337.49s\n",
            "num_batches :  442\n",
            "step :  500 iters_100_time :  44.155232429504395\n",
            "step :  600 iters_100_time :  75.4560341835022\n",
            "step :  700 iters_100_time :  75.15041041374207\n",
            "step :  800 iters_100_time :  74.8247172832489\n",
            "[352/600][442]\n",
            "                    Loss_D: 0.68 Loss_G: 54.04 Time: 333.08s\n",
            "num_batches :  442\n",
            "step :  900 iters_100_time :  12.96884298324585\n",
            "step :  1000 iters_100_time :  75.19675660133362\n",
            "step :  1100 iters_100_time :  75.21159672737122\n",
            "step :  1200 iters_100_time :  75.07015442848206\n",
            "step :  1300 iters_100_time :  75.44087791442871\n",
            "[353/600][442]\n",
            "                    Loss_D: 0.02 Loss_G: 46.28 Time: 333.90s\n",
            "num_batches :  442\n",
            "step :  1400 iters_100_time :  55.66168832778931\n",
            "step :  1500 iters_100_time :  75.53922653198242\n",
            "step :  1600 iters_100_time :  75.4135046005249\n",
            "step :  1700 iters_100_time :  74.58757948875427\n",
            "[354/600][442]\n",
            "                    Loss_D: 0.07 Loss_G: 51.57 Time: 332.74s\n",
            "num_batches :  442\n",
            "step :  1800 iters_100_time :  24.81699538230896\n",
            "step :  1900 iters_100_time :  74.78149437904358\n",
            "step :  2000 iters_100_time :  75.34927868843079\n",
            "step :  2100 iters_100_time :  75.42059087753296\n",
            "step :  2200 iters_100_time :  74.76704668998718\n",
            "[355/600][442]\n",
            "                    Loss_D: 1.63 Loss_G: 106.73 Time: 332.69s\n",
            "num_batches :  442\n",
            "step :  2300 iters_100_time :  68.46586537361145\n",
            "step :  2400 iters_100_time :  75.76343536376953\n",
            "step :  2500 iters_100_time :  74.57569813728333\n",
            "step :  2600 iters_100_time :  74.95832228660583\n",
            "[356/600][442]\n",
            "                    Loss_D: 0.44 Loss_G: 71.39 Time: 333.16s\n",
            "num_batches :  442\n",
            "step :  2700 iters_100_time :  36.88225793838501\n",
            "step :  2800 iters_100_time :  75.2708694934845\n",
            "step :  2900 iters_100_time :  75.45433211326599\n",
            "step :  3000 iters_100_time :  75.59624886512756\n",
            "[357/600][442]\n",
            "                    Loss_D: 0.17 Loss_G: 50.27 Time: 333.54s\n",
            "num_batches :  442\n",
            "step :  3100 iters_100_time :  5.376380205154419\n",
            "step :  3200 iters_100_time :  74.4486985206604\n",
            "step :  3300 iters_100_time :  75.79497504234314\n",
            "step :  3400 iters_100_time :  75.24368739128113\n",
            "step :  3500 iters_100_time :  74.39933371543884\n",
            "[358/600][442]\n",
            "                    Loss_D: 0.30 Loss_G: 48.04 Time: 332.48s\n",
            "num_batches :  442\n",
            "step :  3600 iters_100_time :  48.94581484794617\n",
            "step :  3700 iters_100_time :  74.56591606140137\n",
            "step :  3800 iters_100_time :  75.11285638809204\n",
            "step :  3900 iters_100_time :  75.16779637336731\n",
            "[359/600][442]\n",
            "                    Loss_D: 0.20 Loss_G: 50.48 Time: 332.47s\n",
            "num_batches :  442\n",
            "step :  4000 iters_100_time :  17.16563630104065\n",
            "step :  4100 iters_100_time :  74.76780986785889\n",
            "step :  4200 iters_100_time :  75.52039003372192\n",
            "step :  4300 iters_100_time :  75.2064836025238\n",
            "step :  4400 iters_100_time :  74.89912867546082\n",
            "[360/600][442]\n",
            "                    Loss_D: 0.05 Loss_G: 56.00 Time: 332.79s\n",
            "Save G/Ds models.\n",
            "num_batches :  442\n",
            "step :  4500 iters_100_time :  61.241902112960815\n",
            "step :  4600 iters_100_time :  74.86780047416687\n",
            "step :  4700 iters_100_time :  75.63764643669128\n",
            "step :  4800 iters_100_time :  75.21414804458618\n",
            "[361/600][442]\n",
            "                    Loss_D: 0.04 Loss_G: 60.16 Time: 334.11s\n",
            "num_batches :  442\n",
            "step :  4900 iters_100_time :  28.903417825698853\n",
            "step :  5000 iters_100_time :  75.4080102443695\n",
            "step :  5000 iters_5000_time :  104.31151962280273\n",
            "CURRENT WORKING DIRCTORY :  /content/AttnGAN/code\n",
            "keyTime |||||||||||||||||||||||||||||||\n",
            "build_super_images_time :  7.028107166290283\n",
            "KeyTime |||||||||||||||||||||||||||||||\n",
            "CURRENT WORKING DIRCTORY :  /content/AttnGAN/code\n",
            "keyTime |||||||||||||||||||||||||||||||\n",
            "build_super_images_time :  26.461076498031616\n",
            "KeyTime |||||||||||||||||||||||||||||||\n",
            "CURRENT WORKING DIRCTORY :  /content/AttnGAN/code\n",
            "keyTime |||||||||||||||||||||||||||||||\n",
            "build_super_images_time :  204.64797949790955\n",
            "KeyTime |||||||||||||||||||||||||||||||\n",
            "step :  5100 iters_100_time :  335.6424169540405\n",
            "step :  5200 iters_100_time :  74.98279595375061\n",
            "step :  5300 iters_100_time :  74.86305284500122\n",
            "[362/600][442]\n",
            "                    Loss_D: 0.04 Loss_G: 62.95 Time: 593.34s\n",
            "num_batches :  442\n",
            "step :  5400 iters_100_time :  73.15877747535706\n",
            "step :  5500 iters_100_time :  75.69506525993347\n",
            "step :  5600 iters_100_time :  75.392174243927\n",
            "step :  5700 iters_100_time :  74.61156153678894\n",
            "[363/600][442]\n",
            "                    Loss_D: 0.15 Loss_G: 47.57 Time: 333.73s\n",
            "num_batches :  442\n",
            "step :  5800 iters_100_time :  41.739845514297485\n",
            "step :  5900 iters_100_time :  75.32715129852295\n",
            "step :  6000 iters_100_time :  75.00701808929443\n",
            "step :  6100 iters_100_time :  75.99163794517517\n",
            "[364/600][442]\n",
            "                    Loss_D: 0.26 Loss_G: 63.83 Time: 334.63s\n",
            "num_batches :  442\n",
            "step :  6200 iters_100_time :  9.772260427474976\n",
            "step :  6300 iters_100_time :  75.41409373283386\n",
            "step :  6400 iters_100_time :  75.5185112953186\n",
            "step :  6500 iters_100_time :  75.49393725395203\n",
            "step :  6600 iters_100_time :  74.73727226257324\n",
            "[365/600][442]\n",
            "                    Loss_D: 0.12 Loss_G: 49.13 Time: 334.06s\n",
            "num_batches :  442\n",
            "step :  6700 iters_100_time :  53.72491383552551\n",
            "step :  6800 iters_100_time :  75.72219204902649\n",
            "step :  6900 iters_100_time :  75.25587129592896\n",
            "step :  7000 iters_100_time :  75.74167490005493\n",
            "[366/600][442]\n",
            "                    Loss_D: 1.01 Loss_G: 54.65 Time: 335.12s\n",
            "num_batches :  442\n",
            "step :  7100 iters_100_time :  22.18199324607849\n",
            "step :  7200 iters_100_time :  75.5177104473114\n",
            "step :  7300 iters_100_time :  75.7932960987091\n",
            "step :  7400 iters_100_time :  76.0683479309082\n",
            "step :  7500 iters_100_time :  75.17321133613586\n",
            "[367/600][442]\n",
            "                    Loss_D: 0.50 Loss_G: 48.31 Time: 335.67s\n",
            "num_batches :  442\n",
            "step :  7600 iters_100_time :  66.02730631828308\n",
            "step :  7700 iters_100_time :  76.01005721092224\n",
            "step :  7800 iters_100_time :  75.07347989082336\n",
            "step :  7900 iters_100_time :  74.73541307449341\n",
            "[368/600][442]\n",
            "                    Loss_D: 0.33 Loss_G: 53.47 Time: 334.91s\n",
            "num_batches :  442\n",
            "step :  8000 iters_100_time :  34.03260564804077\n",
            "step :  8100 iters_100_time :  75.16468811035156\n",
            "step :  8200 iters_100_time :  75.0441575050354\n",
            "step :  8300 iters_100_time :  75.83212089538574\n",
            "[369/600][442]\n",
            "                    Loss_D: 2.37 Loss_G: 72.57 Time: 334.88s\n",
            "num_batches :  442\n",
            "step :  8400 iters_100_time :  2.3694915771484375\n",
            "step :  8500 iters_100_time :  75.08566212654114\n",
            "step :  8600 iters_100_time :  75.13559699058533\n",
            "step :  8700 iters_100_time :  76.27562141418457\n",
            "step :  8800 iters_100_time :  75.71183061599731\n",
            "[370/600][442]\n",
            "                    Loss_D: 0.42 Loss_G: 58.44 Time: 335.27s\n",
            "Save G/Ds models.\n",
            "num_batches :  442\n",
            "step :  8900 iters_100_time :  46.2310574054718\n",
            "step :  9000 iters_100_time :  76.06017732620239\n",
            "step :  9100 iters_100_time :  75.4311728477478\n",
            "step :  9200 iters_100_time :  76.13389921188354\n",
            "[371/600][442]\n",
            "                    Loss_D: 0.21 Loss_G: 48.10 Time: 336.66s\n",
            "num_batches :  442\n",
            "step :  9300 iters_100_time :  14.55646276473999\n",
            "step :  9400 iters_100_time :  75.9390799999237\n",
            "step :  9500 iters_100_time :  75.86309671401978\n",
            "step :  9600 iters_100_time :  75.95542430877686\n",
            "step :  9700 iters_100_time :  75.2343475818634\n",
            "[372/600][442]\n",
            "                    Loss_D: 0.07 Loss_G: 56.15 Time: 335.98s\n",
            "num_batches :  442\n",
            "step :  9800 iters_100_time :  58.11265826225281\n",
            "step :  9900 iters_100_time :  76.59071397781372\n",
            "step :  10000 iters_100_time :  75.50541710853577\n",
            "step :  10000 iters_5000_time :  210.20889806747437\n",
            "CURRENT WORKING DIRCTORY :  /content/AttnGAN/code\n",
            "keyTime |||||||||||||||||||||||||||||||\n",
            "build_super_images_time :  7.254011869430542\n",
            "KeyTime |||||||||||||||||||||||||||||||\n",
            "CURRENT WORKING DIRCTORY :  /content/AttnGAN/code\n",
            "keyTime |||||||||||||||||||||||||||||||\n",
            "build_super_images_time :  27.54320979118347\n",
            "KeyTime |||||||||||||||||||||||||||||||\n",
            "CURRENT WORKING DIRCTORY :  /content/AttnGAN/code\n",
            "keyTime |||||||||||||||||||||||||||||||\n",
            "build_super_images_time :  212.19259548187256\n",
            "KeyTime |||||||||||||||||||||||||||||||\n",
            "step :  10100 iters_100_time :  345.16103982925415\n",
            "[373/600][442]\n",
            "                    Loss_D: 1.12 Loss_G: 77.58 Time: 605.98s\n",
            "num_batches :  442\n",
            "step :  10200 iters_100_time :  25.928497076034546\n",
            "step :  10300 iters_100_time :  75.69887518882751\n",
            "step :  10400 iters_100_time :  76.3397159576416\n",
            "step :  10500 iters_100_time :  75.78451228141785\n",
            "step :  10600 iters_100_time :  75.72760152816772\n",
            "[374/600][442]\n",
            "                    Loss_D: 0.05 Loss_G: 67.86 Time: 335.89s\n",
            "num_batches :  442\n",
            "step :  10700 iters_100_time :  71.2698860168457\n",
            "step :  10800 iters_100_time :  76.04863262176514\n",
            "step :  10900 iters_100_time :  75.83427810668945\n",
            "step :  11000 iters_100_time :  76.17002940177917\n",
            "[375/600][442]\n",
            "                    Loss_D: 0.03 Loss_G: 50.81 Time: 337.45s\n",
            "num_batches :  442\n",
            "step :  11100 iters_100_time :  38.93178200721741\n",
            "step :  11200 iters_100_time :  76.04985451698303\n",
            "step :  11300 iters_100_time :  75.75221467018127\n",
            "step :  11400 iters_100_time :  75.7733166217804\n",
            "[376/600][442]\n",
            "                    Loss_D: 0.01 Loss_G: 79.71 Time: 336.64s\n",
            "num_batches :  442\n",
            "step :  11500 iters_100_time :  6.545578479766846\n",
            "step :  11600 iters_100_time :  75.79547953605652\n",
            "step :  11700 iters_100_time :  75.31049609184265\n",
            "step :  11800 iters_100_time :  76.05166411399841\n",
            "step :  11900 iters_100_time :  75.32019472122192\n",
            "[377/600][442]\n",
            "                    Loss_D: 1.83 Loss_G: 83.11 Time: 334.96s\n",
            "num_batches :  442\n",
            "step :  12000 iters_100_time :  50.48209238052368\n",
            "step :  12100 iters_100_time :  76.00315451622009\n",
            "step :  12200 iters_100_time :  75.66803407669067\n",
            "step :  12300 iters_100_time :  75.18418145179749\n",
            "[378/600][442]\n",
            "                    Loss_D: 0.08 Loss_G: 65.95 Time: 335.33s\n",
            "num_batches :  442\n",
            "step :  12400 iters_100_time :  18.808528184890747\n",
            "step :  12500 iters_100_time :  75.8345263004303\n",
            "step :  12600 iters_100_time :  74.90180683135986\n",
            "step :  12700 iters_100_time :  76.04778861999512\n",
            "step :  12800 iters_100_time :  76.43801760673523\n",
            "[379/600][442]\n",
            "                    Loss_D: 0.52 Loss_G: 66.24 Time: 335.77s\n",
            "num_batches :  442\n",
            "step :  12900 iters_100_time :  62.6498122215271\n",
            "step :  13000 iters_100_time :  76.43176007270813\n",
            "step :  13100 iters_100_time :  76.35770869255066\n",
            "step :  13200 iters_100_time :  75.58658742904663\n",
            "[380/600][442]\n",
            "                    Loss_D: 0.04 Loss_G: 61.35 Time: 337.04s\n",
            "Save G/Ds models.\n",
            "num_batches :  442\n",
            "step :  13300 iters_100_time :  31.202097177505493\n",
            "step :  13400 iters_100_time :  76.01010084152222\n",
            "step :  13500 iters_100_time :  75.31713652610779\n",
            "step :  13600 iters_100_time :  75.37488079071045\n",
            "step :  13700 iters_100_time :  75.88142776489258\n",
            "[381/600][442]\n",
            "                    Loss_D: 0.27 Loss_G: 59.60 Time: 335.95s\n",
            "num_batches :  442\n",
            "step :  13800 iters_100_time :  74.69491744041443\n",
            "step :  13900 iters_100_time :  75.68916606903076\n",
            "step :  14000 iters_100_time :  76.4868974685669\n",
            "step :  14100 iters_100_time :  76.30317950248718\n",
            "[382/600][442]\n",
            "                    Loss_D: 0.01 Loss_G: 62.73 Time: 337.11s\n",
            "num_batches :  442\n",
            "step :  14200 iters_100_time :  43.62891602516174\n",
            "step :  14300 iters_100_time :  75.87774634361267\n",
            "step :  14400 iters_100_time :  75.17016172409058\n",
            "step :  14500 iters_100_time :  75.67221093177795\n",
            "[383/600][442]\n",
            "                    Loss_D: 0.19 Loss_G: 57.16 Time: 336.75s\n",
            "num_batches :  442\n",
            "step :  14600 iters_100_time :  11.25625205039978\n",
            "step :  14700 iters_100_time :  75.49087595939636\n",
            "step :  14800 iters_100_time :  75.77261710166931\n",
            "step :  14900 iters_100_time :  76.47537183761597\n",
            "step :  15000 iters_100_time :  75.78918385505676\n",
            "step :  15000 iters_5000_time :  314.78449988365173\n",
            "CURRENT WORKING DIRCTORY :  /content/AttnGAN/code\n",
            "keyTime |||||||||||||||||||||||||||||||\n",
            "build_super_images_time :  6.88337516784668\n",
            "KeyTime |||||||||||||||||||||||||||||||\n",
            "CURRENT WORKING DIRCTORY :  /content/AttnGAN/code\n",
            "keyTime |||||||||||||||||||||||||||||||\n",
            "build_super_images_time :  26.4010169506073\n",
            "KeyTime |||||||||||||||||||||||||||||||\n",
            "CURRENT WORKING DIRCTORY :  /content/AttnGAN/code\n",
            "keyTime |||||||||||||||||||||||||||||||\n",
            "build_super_images_time :  196.34716391563416\n",
            "KeyTime |||||||||||||||||||||||||||||||\n",
            "[384/600][442]\n",
            "                    Loss_D: 0.46 Loss_G: 67.03 Time: 586.65s\n",
            "num_batches :  442\n",
            "step :  15100 iters_100_time :  55.70279026031494\n",
            "step :  15200 iters_100_time :  76.28867316246033\n",
            "step :  15300 iters_100_time :  76.27817559242249\n",
            "step :  15400 iters_100_time :  76.42366576194763\n",
            "[385/600][442]\n",
            "                    Loss_D: 0.10 Loss_G: 54.01 Time: 338.24s\n",
            "num_batches :  442\n",
            "step :  15500 iters_100_time :  22.879778623580933\n",
            "step :  15600 iters_100_time :  76.51402759552002\n",
            "step :  15700 iters_100_time :  76.17330932617188\n",
            "step :  15800 iters_100_time :  76.72072315216064\n",
            "step :  15900 iters_100_time :  76.19282007217407\n",
            "[386/600][442]\n",
            "                    Loss_D: 0.05 Loss_G: 53.11 Time: 337.92s\n",
            "num_batches :  442\n",
            "step :  16000 iters_100_time :  67.95130825042725\n",
            "step :  16100 iters_100_time :  75.21534848213196\n",
            "step :  16200 iters_100_time :  75.55389189720154\n",
            "step :  16300 iters_100_time :  76.38271427154541\n",
            "[387/600][442]\n",
            "                    Loss_D: 2.81 Loss_G: 92.01 Time: 336.81s\n",
            "num_batches :  442\n",
            "step :  16400 iters_100_time :  35.170793771743774\n",
            "step :  16500 iters_100_time :  75.74709725379944\n",
            "step :  16600 iters_100_time :  76.53655672073364\n",
            "step :  16700 iters_100_time :  75.34649324417114\n",
            "[388/600][442]\n",
            "                    Loss_D: 0.04 Loss_G: 68.22 Time: 336.39s\n",
            "num_batches :  442\n",
            "step :  16800 iters_100_time :  3.7980103492736816\n",
            "step :  16900 iters_100_time :  75.94914937019348\n",
            "step :  17000 iters_100_time :  75.37025737762451\n",
            "step :  17100 iters_100_time :  75.8994836807251\n",
            "step :  17200 iters_100_time :  76.5087022781372\n",
            "[389/600][442]\n",
            "                    Loss_D: 0.01 Loss_G: 63.35 Time: 336.95s\n",
            "num_batches :  442\n",
            "step :  17300 iters_100_time :  47.19059085845947\n",
            "step :  17400 iters_100_time :  76.20922350883484\n",
            "step :  17500 iters_100_time :  75.60043931007385\n",
            "step :  17600 iters_100_time :  75.75682330131531\n",
            "[390/600][442]\n",
            "                    Loss_D: 1.05 Loss_G: 43.50 Time: 336.83s\n",
            "Save G/Ds models.\n",
            "num_batches :  442\n",
            "step :  17700 iters_100_time :  15.981949806213379\n",
            "step :  17800 iters_100_time :  76.15320873260498\n",
            "step :  17900 iters_100_time :  76.47633409500122\n",
            "step :  18000 iters_100_time :  76.38190913200378\n",
            "step :  18100 iters_100_time :  75.5205774307251\n",
            "[391/600][442]\n",
            "                    Loss_D: 0.06 Loss_G: 67.83 Time: 337.55s\n",
            "num_batches :  442\n",
            "step :  18200 iters_100_time :  60.040427923202515\n",
            "step :  18300 iters_100_time :  76.70513129234314\n",
            "step :  18400 iters_100_time :  76.15114068984985\n",
            "step :  18500 iters_100_time :  76.11758708953857\n",
            "[392/600][442]\n",
            "                    Loss_D: 0.02 Loss_G: 48.86 Time: 338.42s\n",
            "num_batches :  442\n",
            "step :  18600 iters_100_time :  28.270062685012817\n",
            "step :  18700 iters_100_time :  74.83969712257385\n",
            "step :  18800 iters_100_time :  76.61932110786438\n",
            "step :  18900 iters_100_time :  75.67254495620728\n",
            "step :  19000 iters_100_time :  75.73614454269409\n",
            "[393/600][442]\n",
            "                    Loss_D: 0.88 Loss_G: 91.94 Time: 336.04s\n",
            "num_batches :  442\n",
            "step :  19100 iters_100_time :  72.76926898956299\n",
            "step :  19200 iters_100_time :  76.87849855422974\n",
            "step :  19300 iters_100_time :  76.46445488929749\n",
            "step :  19400 iters_100_time :  76.35794925689697\n",
            "[394/600][442]\n",
            "                    Loss_D: 0.25 Loss_G: 56.62 Time: 339.82s\n",
            "num_batches :  442\n",
            "step :  19500 iters_100_time :  40.251354694366455\n",
            "step :  19600 iters_100_time :  75.07671093940735\n",
            "step :  19700 iters_100_time :  76.49236249923706\n",
            "step :  19800 iters_100_time :  75.78373122215271\n",
            "[395/600][442]\n",
            "                    Loss_D: 0.01 Loss_G: 62.22 Time: 336.27s\n",
            "num_batches :  442\n",
            "step :  19900 iters_100_time :  8.408398866653442\n",
            "step :  20000 iters_100_time :  76.51749467849731\n",
            "step :  20000 iters_5000_time :  84.92596292495728\n",
            "CURRENT WORKING DIRCTORY :  /content/AttnGAN/code\n",
            "keyTime |||||||||||||||||||||||||||||||\n",
            "build_super_images_time :  7.067493200302124\n",
            "KeyTime |||||||||||||||||||||||||||||||\n",
            "CURRENT WORKING DIRCTORY :  /content/AttnGAN/code\n",
            "keyTime |||||||||||||||||||||||||||||||\n",
            "build_super_images_time :  27.0156192779541\n",
            "KeyTime |||||||||||||||||||||||||||||||\n",
            "CURRENT WORKING DIRCTORY :  /content/AttnGAN/code\n",
            "keyTime |||||||||||||||||||||||||||||||\n",
            "build_super_images_time :  216.16100811958313\n",
            "KeyTime |||||||||||||||||||||||||||||||\n",
            "step :  20100 iters_100_time :  348.70841360092163\n",
            "step :  20200 iters_100_time :  75.82937741279602\n",
            "step :  20300 iters_100_time :  75.8969714641571\n",
            "[396/600][442]\n",
            "                    Loss_D: 0.62 Loss_G: 69.01 Time: 610.12s\n",
            "num_batches :  442\n",
            "step :  20400 iters_100_time :  53.03449010848999\n",
            "step :  20500 iters_100_time :  75.37231850624084\n",
            "step :  20600 iters_100_time :  75.95078992843628\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}